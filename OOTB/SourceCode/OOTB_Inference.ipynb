{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1J8Gkd2L6eXuZZ4QgwsvhlRqqNxuynEI-","timestamp":1677571721571}],"mount_file_id":"1J8Gkd2L6eXuZZ4QgwsvhlRqqNxuynEI-","authorship_tag":"ABX9TyPrSIdXd1zIjwMAufkiEDwc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-gb21iG2cx-4"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import glob"]},{"cell_type":"code","source":["# Function that takes in the JSON File and return a list of tuples where each tuple is (Run_Config, Omnetpp.ini, TargetsDictionary)\n","\n","def JSONExtractor(JSON):\n","  runINI = []\n","  delINI = []\n","  ignore = []\n","  disk = \"'peakdiskspaceusedbytes'\"\n","  simRAM = \"'peakramusedsimbytes'\"\n","  resRAM = \"'peakramusedresultsbytes'\"\n","  time = \"'totaljobclocktimesec'\"\n","\n","  for i in range(len(JSON)):\n","    \n","    # Checking if the Simulation_Completion and Results_Parsing_Completion are 100% and current state = 'COMPLETED'\n","    if ((JSON.simulations[i]['meta']['sim_completed_perc']==100) and (JSON.simulations[i]['meta']['results_completed_perc']==100) and (JSON.simulations[i]['meta']['current_state']=='COMPLETED')):\n","\n","      # Checking if simulation contains meta data with 'sim_runtime_stats' as key (if not present 'totaljobclocktimesec' param can't be found, so discarding them)\n","      if 'sim_runtime_stats' in JSON.simulations[i]['meta']:\n","\n","        # Fetching the all 4 targets variables from meta data of the simulation \n","        dicts = {'peakdiskspaceusedbytes':re.compile(disk + r'\\,\\s([0-9\\.]+)').findall(str(JSON.simulations[i]['meta']['sim_runtime_stats'])),\n","             'peakramusedsimbytes':re.compile(simRAM + r'\\,\\s([0-9\\.]+)').findall(str(JSON.simulations[i]['meta']['sim_runtime_stats'])),\n","             'peakramusedresultsbytes':re.compile(resRAM + r'\\,\\s([0-9\\.]+)').findall(str(JSON.simulations[i]['meta']['sim_runtime_stats'])),                                                                      \n","             'totaljobclocktimesec':re.compile(time + r'\\,\\s([0-9\\.]+)').findall(str(JSON.simulations[i]['meta']['sim_runtime_stats'])) }\n","        \n","        # Appending Run_Config, Omnetpp.ini and Targets dictionary as a tuple in runINI list\n","        runINI.append((JSON.simulations[i]['runconfig'],JSON.simulations[i]['omnetppini'],dicts))\n","      else:\n","        ignore.append((JSON.simulations[i]['runconfig'],JSON.simulations[i]['omnetppini']))\n","    else:\n","      # print(i)\n","      # Appending info about incomplete simulations\n","      delINI.append((JSON.simulations[i]['runconfig'],JSON.simulations[i]['omnetppini'],dicts))\n","\n","  return runINI"],"metadata":{"id":"HhqB5qXZdJkc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function that takes in Run_Config, .ini, TargetsDictionary and returns a dictionary of all .ini parameters \n","# of a simulation along with their corresponding targets\n","\n","def FeatureExtractor(runconfig,omnetppini,targets):\n","  if not runconfig == 'General':\n","    # print(runconfig)\n","    # replacing multiple \\r and \\n with null and \\n resp. and splitting on \\n basis\n","    splitINI = re.sub(r'[\\n]+','\\n ',re.sub(r'[\\r]+','',omnetppini)).strip().split('\\n')\n","\n","    # INI filtering (removes any redundant commands/comments starting with '#' in the ini) \n","    refreshINI = ''.join([line.strip() for line in splitINI if not line.strip().startswith('#')])\n","\n","    # RunConfig Filtering (Filter INI for the specific RunConfig) --- case where mutiple Run_Configs \n","    # are present in same ini file\n","    configFiltered = re.compile(runconfig + r'[\\'\\]\\n\\#\\s\\w\\*\\.=\\(\\)\\\"\\,\\-\\/\\+]+').findall(refreshINI)\n","\n","    # Extracting every ini param and converting to dict\n","    paramDict = dict(re.findall(r'\\*\\*\\.([a-zA-Z\\.]+) = ([A-Za-z0-9\\\"\\.\\/\\-]+)',str(configFiltered)))\n","    if 'numNodes' not in paramDict.keys():\n","      x = re.findall(r'\\*\\*\\.(numNodes) = ([0-9]+)',refreshINI)[0]\n","      paramDict[x[0]] = x[1]\n","\n","    # Combining .ini parameters dictionary with TargetsDictionary\n","    paramDict.update(targets)\n","\n","    return paramDict"],"metadata":{"id":"LgJnviO_dL78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["JSONDrives = glob.glob('/content/drive/MyDrive/OOTB/simulation-meta_26_02_2023.json')\n","print(len(JSONDrives))\n","JSONDrives"],"metadata":{"id":"oK2h8i7fdNm5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["finalDic = []\n","inis = []\n","runconfig = []\n","for path in JSONDrives:\n","  try:\n","    json = pd.read_json(path)\n","    print(JSONDrives[-1])\n","    JsonFiltered = JSONExtractor(json)\n","    # print(\"Initial Length of JSON\",len(json),\"Final Length of Filtered JSON\",len(JsonFiltered))\n","\n","    # ParamsList = []\n","    for rc,ini,targets in JsonFiltered:\n","      # Feature Extractor function returns .ini params and targets as dictionary\n","      dic = FeatureExtractor(rc,ini,targets)\n","      if bool(dic):\n","        # ParamsList.append(dic)\n","        inis.append(ini)\n","        runconfig.append(rc)\n","        finalDic.append(dic)\n","  except:\n","    # print(path)\n","    print(\"ss\")"],"metadata":{"id":"SeX7MyESdVgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Numericals = ['app_dataGenerationInterval','constraintAreaMaxX','constraintAreaMaxY','mobility_noOfLocations','mobility_Hosts','mobility_speed','numNodes','app_dataSizeInBytes','forwarding_maximumCacheSize','mobility_nodeId']\n","Categoricals = ['applicationLayer','forwardingLayer','linkLayer','mobilityType']\n","Targets = ['peakdiskspaceusedbytes','peakramusedsimbytes','peakramusedresultsbytes','totaljobclocktimesec']"],"metadata":{"id":"9ZPKogrjqH6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["first = pd.DataFrame(finalDic)\n","first"],"metadata":{"id":"VrbmBH1adaWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_null = first[first['forwardingLayer'].isna()]\n","for_null"],"metadata":{"id":"Rdbx6OOyeFyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["second = first[~first['forwardingLayer'].isna()]\n","second"],"metadata":{"id":"O9tn9KDbpeZn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["second.columns = second.columns.str.replace('.','_',regex=False)\n","\n","remove_cols = ['app_popularityAssignmentPercentage','app_usedRNG','forwarding_antiEntropyInterval','forwarding_maximumHopCount','forwarding_usedRNG','link_wirelessRange','link_neighbourScanInterval','link_bandwidthBitRate','link_wirelessHeaderSize','link_usedRNG','constraintAreaMinX','constraintAreaMinY','constraintAreaMinZ','constraintAreaMaxZ','updateInterval','mobility_initFromDisplayString','mobility_popularityDecisionThreshold','mobility_returnHomePercentage','mobility_neighbourLocationLimit','mobility_radius','mobility_alpha','mobility_waitTime','mobility_usedRNG','app_dataGenerationIntervalMode','app_trafficInfoPath','forwarding_spraywaitFlavour','forwarding_noDuplicate','mobility_traceFile','link_contactTracesPath','forwarding_broadcastRRS','forwarding_sendOnNeighReportingFrequency','forwarding_sendFrequencyWhenNotOnNeighFrequency','forwarding_pEncounterMax','forwarding_pEncounterFirst','forwarding_pFirstThreshold','forwarding_alpha','forwarding_beta','forwarding_gamma','forwarding_delta','forwarding_standardTimeInterval','forwarding_agingInterval','forwarding_neighbourhoodChangeSignificanceThreshold','forwarding_coolOffDuration','forwarding_learningConst','forwarding_backoffTimerIncrementFactor','app_specificDestination','app_specificDestinationNodeName','app_ttl','forwarding_useTTL']\n","\n","leftover_cols = [col for col in second.columns if col not in remove_cols]\n","\n","# Different coulmn categories\n","Numericals = ['app_dataGenerationInterval','constraintAreaMaxX','constraintAreaMaxY','mobility_noOfLocations','mobility_Hosts','mobility_speed','numNodes','app_dataSizeInBytes','forwarding_maximumCacheSize','mobility_nodeId']\n","Categoricals = ['applicationLayer','forwardingLayer','linkLayer','mobilityType']\n","Targets = ['peakdiskspaceusedbytes','peakramusedsimbytes','peakramusedresultsbytes','totaljobclocktimesec']\n","\n","# Seprating dataframe with required columns\n","third = second[leftover_cols]"],"metadata":{"id":"gGejRhb8p7fB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["third.mobilityType = third.mobilityType.fillna(\"ContactTraces\")\n","third.mobility_nodeId = third.mobility_nodeId.fillna('0').str.replace('-1','1')\n","third.constraintAreaMaxX = third.constraintAreaMaxX.fillna('0')\n","third.constraintAreaMaxY = third.constraintAreaMaxY.fillna('0')\n","third.mobility_noOfLocations = third.mobility_noOfLocations.fillna('0')\n","third.mobility_Hosts = third.mobility_Hosts.fillna('0')\n","third.mobility_speed = third.mobility_speed.fillna('0')\n","third.forwarding_maximumCacheSize = third.forwarding_maximumCacheSize.fillna('0')"],"metadata":{"id":"yXc0294aqlWh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_hots = pd.get_dummies(third[Categoricals],drop_first=True)\n","fourth = pd.concat([third,one_hots],axis=1)\n","fourth = fourth.drop(Categoricals,axis=1)"],"metadata":{"id":"3O646u6Tuj_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Changing Numerical features from strings to integers\n","fourth['app_dataGenerationInterval'] =  fourth.app_dataGenerationInterval.apply(lambda x : x.replace('s','') if 's' in x else x)\n","fourth['constraintAreaMaxX'] =  fourth.constraintAreaMaxX.apply(lambda x : x.replace('m','') if 'm' in x else x)\n","fourth['constraintAreaMaxY'] =  fourth.constraintAreaMaxY.apply(lambda x : x.replace('m','') if 'm' in x else x)\n","fourth['mobility_speed'] =  fourth.mobility_speed.apply(lambda x : x.replace('mps','') if 'mps' in x else x)\n","def for_maxCache(cache):\n","  if 'bytes' in str(cache):\n","    return cache.replace('bytes','')\n","  elif 'byte' in str(cache):\n","    return cache.replace('byte','')\n","  else:\n","    return cache\n","fourth['forwarding_maximumCacheSize'] =  fourth.forwarding_maximumCacheSize.apply(lambda x : for_maxCache(x))"],"metadata":{"id":"kNDn6EYquyz1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Separating features from the dataset and changing their datatype\n","features = fourth.drop(Targets,axis=1)\n","fourth[features.columns] = features.apply(pd.to_numeric)\n","\n","# Changing Target variables from string to integers\n","def target_list(target):\n","  if type(target) is list:\n","    return pd.to_numeric(target[0],errors='coerce')\n","for col in Targets:\n","  fourth[col] = fourth[col].apply(lambda x: target_list(x))"],"metadata":{"id":"29yL611uwQET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fourth.columns = fourth.columns.str.replace('\"','')\n","fourth = fourth.reset_index(drop=True)\n","fourth"],"metadata":{"id":"m97N3-oLwv9Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the dataset into the drive\n","# fourth.to_csv('/content/drive/MyDrive/OOTB/DataWithDummy_26_02_2023.csv',index=False)"],"metadata":{"id":"l5cPvGZzxKw9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pre-Processing DATA"],"metadata":{"id":"RocQTD1Ux2a2"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import stats\n","from scipy.stats import norm, skew, johnsonsu, lognorm\n","from scipy.special import boxcox1p,inv_boxcox1p"],"metadata":{"id":"QYcitoFbzGW-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fifth.columns"],"metadata":{"id":"9BpkpTZMyT9A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MobilityNodeID=1 for BONN_MOTION_MOBILITY simualtions\n","sns.histplot(fifth['mobility_nodeId'],stat='count')"],"metadata":{"id":"A9Fiwz-cysUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Skewness of all features\n","# Check the skew of all numerical features\n","skewed_feats = fifth[Numericals].apply(lambda x: skew(x)).sort_values(ascending=False)\n","print(\"\\nSkew in numerical features: \\n\")\n","skewness = pd.DataFrame({'Skew' :skewed_feats})\n","skewness"],"metadata":{"id":"g01WtuTEzxQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Applying BoxCox transforms\n","skewness = skewness[abs(skewness['Skew'].values) > 0.75]\n","print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n","skewed_features = skewness.index\n","lam = 0.15\n","for feat in skewed_features:\n","  print(feat)\n","  #all_data[feat] += 1\n","  fifth[feat] = boxcox1p(fifth[feat], lam)"],"metadata":{"id":"IKCwYUMf05FK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the NEW skew of all numerical features\n","skewed_feats = fifth[Numericals].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n","print(\"\\nSkew in numerical features: \\n\")\n","skewness = pd.DataFrame({'Skew' :skewed_feats})\n","skewness"],"metadata":{"id":"TEgxDW0J1AeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sixth = fifth.copy(deep=True)\n","sixth"],"metadata":{"id":"TfUBTL8u25Oc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Modelling"],"metadata":{"id":"KQsmbdzy1dmn"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n","from keras.models import Sequential\n","from keras.layers import Dense,BatchNormalization,Dropout\n","from keras.optimizers import Adam,SGD,RMSprop,Adadelta,Adagrad,Adamax,Nadam,Ftrl\n","from keras.callbacks import EarlyStopping,ModelCheckpoint\n","from keras.wrappers.scikit_learn import KerasClassifier,KerasRegressor\n","from keras.utils import plot_model\n","from sklearn.metrics import make_scorer,accuracy_score\n","from sklearn.model_selection import StratifiedKFold,KFold\n","from keras.layers import LeakyReLU\n","from keras.losses import mean_squared_error,mean_squared_logarithmic_error,mean_absolute_error,huber_loss\n","from keras.callbacks import LearningRateScheduler,EarlyStopping\n","from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n","import xgboost as xgb\n","import lightgbm as lgb\n","from sklearn.base import BaseEstimator, TransformerMixin,RegressorMixin,clone\n","LeakyReLU = LeakyReLU(alpha=0.1)\n","import warnings\n","import time\n","import pickle\n","warnings.filterwarnings('ignore')\n","pd.set_option('display.max_columns',None)"],"metadata":{"id":"JSWkum7d1XHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = sixth.drop(Targets,axis=1)\n","y = boxcox1p(sixth[Targets],0.20)\n","\n","x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.1,shuffle=True)\n","\n","train_scalar1 = StandardScaler()\n","\n","train_scalar2 = StandardScaler()\n","\n","x_train_scaled = pd.DataFrame(data=train_scalar1.fit_transform(x_train),columns=x_train.columns,index=x_train.index)\n","y_train_scaled = pd.DataFrame(train_scalar2.fit_transform(y_train),columns=y_train.columns,index=y_train.index)\n","\n","x_test_scaled = pd.DataFrame(data=train_scalar1.transform(x_test),columns=x_test.columns,index=x_test.index)\n","y_test_scaled = pd.DataFrame(train_scalar2.transform(y_test),columns=y_test.columns,index=y_test.index)\n","\n","x_train_scaled,x_val_scaled,y_train_scaled,y_val_scaled = train_test_split(x_train_scaled,y_train_scaled,test_size=0.1)"],"metadata":{"id":"ApalYz_k1sd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Samples in Train set : \", len(x_train_scaled))\n","print(\"Samples in Validation set : \", len(x_val_scaled))\n","print(\"Samples in Test set : \", len(x_test_scaled))"],"metadata":{"id":"9p7v6wPV2AVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Shape of Train set : \", x_train_scaled.shape)\n","print(\"Shape of Validation set : \", x_val_scaled.shape)\n","print(\"Shape of Test set : \", x_test_scaled.shape)"],"metadata":{"id":"qd15K8CB2V0l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Target distribution in Train, Validation and Test sets\n","\n","# sns.distplot(y_train_scaled.iloc[:,1],hist=False,label='Train data')\n","# # sns.distplot(y_test.iloc[:,1],hist=False, label= 'Test data')\n","# sns.distplot(y_val_scaled.iloc[:,1],hist=False, label= 'Validation data')\n","# plt.xlabel('Peak DISK Usage (scaled)')\n","# plt.legend()"],"metadata":{"id":"hbsdrP4t3aL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Stats of targets in test dataset\n","print(len(y_test))\n","print(y_test.iloc[:,0].mean())\n","print(y_test.iloc[:,0].std())"],"metadata":{"id":"I4lRZlyC32w4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["MODEL 1 (NEURAL NETWORK)"],"metadata":{"id":"gsZC-3hi4E8R"}},{"cell_type":"code","source":["# SINGLE NEURAL NET\n","single = Sequential()\n","single.add(Dense(units=40,input_dim=x_train_scaled.shape[1],activation='relu'))\n","single.add(Dropout(0.1))\n","single.add(Dense(20, activation='relu'))\n","single.add(Dropout(0.1))\n","single.add(Dense(12, activation='relu'))\n","single.add(Dropout(0.1))\n","single.add(Dense(8, activation='relu'))\n","single.add(Dropout(0.1))\n","single.add(Dense(4))\n","single.compile(loss='mean_squared_error',optimizer=Adam(learning_rate=0.1),metrics=['mean_squared_error'])\n","\n","def scheduler(epoch, lr):\n","  if epoch < 2:\n","    return lr\n","  else:\n","    return lr * np.exp(-0.1)\n","callbacks = [EarlyStopping(monitor='loss', patience=4),LearningRateScheduler(scheduler)]\n","single.fit(x_train_scaled,y_train_scaled,epochs=100,callbacks=callbacks)"],"metadata":{"id":"Ng2ZNWdgE_xJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SINGLE NEURAL NET\n","single_pred = pd.DataFrame(inv_boxcox1p(train_scalar2.inverse_transform(single.predict(x_test_scaled)),0.20),columns=y_test_scaled.columns,index=y_test_scaled.index)\n","y_test_org  = inv_boxcox1p(y_test,0.20)\n","single_pred.head()"],"metadata":{"id":"P-68V0-0FAx8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PERFORMANCE METRICS of the SINGLE NEURAL NETWORK model\n","y1 = mean_absolute_error(y_test_org.iloc[:,0],single_pred.iloc[:,0])\n","print(y1*(1e-9))\n","y2 = mean_absolute_error(y_test_org.iloc[:,1],single_pred.iloc[:,1])\n","print(y2*(1e-9))\n","y3 = mean_absolute_error(y_test_org.iloc[:,2],single_pred.iloc[:,2])\n","print(y3*(1e-9))\n","y4 = mean_absolute_error(y_test_org.iloc[:,3],single_pred.iloc[:,3])\n","print(y4)"],"metadata":{"id":"lD6m1xLyFAsg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomEstimator(BaseEstimator,TransformerMixin):\n","  def __init__(self):\n","    self.model1 = KerasRegressor(build_fn=self.base1,epochs=100,batch_size=64,verbose=0)\n","    self.model2 = KerasRegressor(build_fn=self.base2,epochs=100,batch_size=64,verbose=0)\n","    self.model3 = KerasRegressor(build_fn=self.base3,epochs=100,batch_size=64,verbose=0)\n","    self.model4 = KerasRegressor(build_fn=self.base4,epochs=100,batch_size=64,verbose=0)\n","\n","  # def fit(self,X,y):\n","  #   history1 = self.model1.fit(X,y[0])\n","  #   history2 = self.model2.fit(X,y[1])\n","  #   history3 = self.model3.fit(X,y[2])\n","  #   history4 = self.model4.fit(X,y[3])\n","  def fit(self,X,y,x_val_scaled,y_val_scaled):\n","    history1 = self.model1.fit(X,y[0],validation_data=(x_val_scaled,y_val_scaled))\n","    history2 = self.model2.fit(X,y[1],validation_data=(x_val_scaled,y_val_scaled))\n","    history3 = self.model3.fit(X,y[2],validation_data=(x_val_scaled,y_val_scaled))\n","    history4 = self.model4.fit(X,y[3],validation_data=(x_val_scaled,y_val_scaled))\n","    return self,[history1,history2,history3,history4]\n","\n","  def predict(self,x_test):\n","    p1 = pd.DataFrame(self.model1.predict(x_test))\n","    p2 = pd.DataFrame(self.model2.predict(x_test))\n","    p3 = pd.DataFrame(self.model3.predict(x_test))\n","    p4 = pd.DataFrame(self.model4.predict(x_test))\n","    pred_df = pd.concat([p1,p2,p3,p4],axis=1)\n","\n","    return pred_df\n","  \n","  def base1(self):\n","    model1 = Sequential()\n","    model1.add(Dense(units=44,input_dim=x_train_scaled.shape[1],activation='relu'))\n","    model1.add(Dropout(0.1))\n","    model1.add(Dense(22, activation='relu'))\n","    model1.add(Dropout(0.1))\n","    model1.add(Dense(12, activation='relu'))\n","    model1.add(Dropout(0.1))\n","    model1.add(Dense(8, activation='relu'))\n","    model1.add(Dropout(0.1))\n","    model1.add(Dense(4, activation='relu'))\n","    model1.add(Dropout(0.1))\n","    model1.add(Dense(1))\n","    model1.compile(loss='mean_squared_error',optimizer=Adam(learning_rate=0.01),metrics=['mean_squared_error'])\n","    return model1\n","\n","  def base2(self):\n","    model2 = Sequential()\n","    model2.add(Dense(units=44,input_dim=x_train_scaled.shape[1],activation='relu'))\n","    model2.add(Dropout(0.1))\n","    model2.add(Dense(22, activation='relu'))\n","    model2.add(Dropout(0.1))\n","    model2.add(Dense(12, activation='relu'))\n","    model2.add(Dropout(0.1))\n","    model2.add(Dense(8, activation='relu'))\n","    model2.add(Dropout(0.1))\n","    model2.add(Dense(4, activation='relu'))\n","    model2.add(Dropout(0.1))\n","    model2.add(Dense(1))\n","    model2.compile(loss='mean_squared_error',optimizer=Adam(learning_rate=0.01),metrics=['mean_squared_error'])\n","    return model2\n","\n","  def base3(self):\n","    model3 = Sequential()\n","    model3.add(Dense(units=44,input_dim=x_train_scaled.shape[1],activation='relu'))\n","    model3.add(Dropout(0.1))\n","    model3.add(Dense(22, activation='relu'))\n","    model3.add(Dropout(0.1))\n","    model3.add(Dense(12, activation='relu'))\n","    model3.add(Dropout(0.1))\n","    model3.add(Dense(8, activation='relu'))\n","    model3.add(Dropout(0.1))\n","    model3.add(Dense(4, activation='relu'))\n","    # model3.add(Dropout(0.1))\n","    model3.add(Dense(1))\n","    model3.compile(loss='mean_squared_error',optimizer=Adam(learning_rate=0.01),metrics=['mean_squared_error'])\n","    return model3\n","\n","  def base4(self):\n","    model4 = Sequential()\n","    model4.add(Dense(units=44,input_dim=x_train_scaled.shape[1],activation='relu'))\n","    model4.add(Dropout(0.1))\n","    model4.add(Dense(22, activation='relu'))\n","    model4.add(Dropout(0.1))\n","    model4.add(Dense(12, activation='relu'))\n","    model4.add(Dropout(0.1))\n","    model4.add(Dense(8, activation='relu'))\n","    model4.add(Dropout(0.1))\n","    model4.add(Dense(4, activation='relu'))\n","    model4.add(Dropout(0.1))\n","    model4.add(Dense(1))\n","    model4.compile(loss='mean_squared_error',optimizer=Adam(learning_rate=0.01),metrics=['mean_squared_error'])\n","    return model4"],"metadata":{"id":"GD-B3DiT39bE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CE = CustomEstimator()\n","start_time = time.time()\n","# _ , histories = CE.fit(x_train_scaled,[y_train_scaled.iloc[:,0].values,y_train_scaled.iloc[:,1].values,y_train_scaled.iloc[:,2].values,y_train_scaled.iloc[:,3].values])\n","_ , histories = CE.fit(x_train_scaled,[y_train_scaled.iloc[:,0].values,y_train_scaled.iloc[:,1].values,y_train_scaled.iloc[:,2].values,y_train_scaled.iloc[:,3].values],x_val_scaled,y_val_scaled)\n","end_time = time.time()\n","print(end_time-start_time)"],"metadata":{"id":"sRNf3FqS4KCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loss = pd.DataFrame(inv_boxcox1p(train_scalar2.inverse_transform(pd.DataFrame(np.hstack((np.array(histories[0].history['loss']).reshape(-1,1),np.array(histories[1].history['loss']).reshape(-1,1),np.array(histories[2].history['loss']).reshape(-1,1),np.array(histories[3].history['loss']).reshape(-1,1))))),0.20)).multiply(1e-9)\n","val_loss = pd.DataFrame(inv_boxcox1p(train_scalar2.inverse_transform(pd.DataFrame(np.hstack((np.array(histories[0].history['val_loss']).reshape(-1,1),np.array(histories[1].history['val_loss']).reshape(-1,1),np.array(histories[2].history['val_loss']).reshape(-1,1),np.array(histories[3].history['val_loss']).reshape(-1,1))))),0.20)).multiply(1e-9)\n","\n","train_loss_time = pd.DataFrame(inv_boxcox1p(train_scalar2.inverse_transform(pd.DataFrame(np.hstack((np.array(histories[0].history['loss']).reshape(-1,1),np.array(histories[1].history['loss']).reshape(-1,1),np.array(histories[2].history['loss']).reshape(-1,1),np.array(histories[3].history['loss']).reshape(-1,1))))),0.20))\n","val_loss_time = pd.DataFrame(inv_boxcox1p(train_scalar2.inverse_transform(pd.DataFrame(np.hstack((np.array(histories[0].history['val_loss']).reshape(-1,1),np.array(histories[1].history['val_loss']).reshape(-1,1),np.array(histories[2].history['val_loss']).reshape(-1,1),np.array(histories[3].history['val_loss']).reshape(-1,1))))),0.20))"],"metadata":{"id":"lW3UyQty4kzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Epoch vs Loss Plots for Target Variables\n","\n","plt.rcParams[\"figure.figsize\"] = (20,10)\n","fig,axs = plt.subplots(2,2)\n","\n","axs[0,0].plot(train_loss.iloc[:,0])\n","axs[0,0].plot(val_loss.iloc[:,0])\n","axs[0,0].set_title('Peak Disk Usage')\n","axs[0,0].set_xlabel('epochs')\n","axs[0,0].set_ylabel('MSE Loss in GB')\n","axs[0,0].legend(['train', 'val'], loc='upper left')\n","\n","axs[0,1].plot(train_loss.iloc[:,1])\n","axs[0,1].plot(val_loss.iloc[:,1])\n","axs[0,1].set_title('Peak Simulation RAM')\n","axs[0,1].set_xlabel('epochs')\n","axs[0,1].set_ylabel('MSE Loss in GB')\n","axs[0,1].legend(['train', 'val'], loc='upper left')\n","\n","axs[1,0].plot(train_loss.iloc[:,2])\n","axs[1,0].plot(val_loss.iloc[:,2])\n","axs[1,0].set_title('Peak Results Parsing RAM')\n","axs[1,0].set_xlabel('epochs')\n","axs[1,0].set_ylabel('MSE Loss in GB')\n","axs[1,0].legend(['train', 'val'], loc='upper left')\n","\n","axs[1,1].plot(train_loss_time.iloc[:,3])\n","axs[1,1].plot(val_loss_time.iloc[:,3])\n","axs[1,1].set_title('Total Time Taken')\n","axs[1,1].set_xlabel('epochs')\n","axs[1,1].set_ylabel('MSE Loss in Secs')\n","axs[1,1].legend(['train', 'val'], loc='upper left')\n","\n","# plt.set_title('Training/Validation plots')\n","fig.show()"],"metadata":{"id":"0qxpvzfv4P2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = CE.predict(x_test_scaled)"],"metadata":{"id":"8WTGJCC_4tnS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CE_df = pd.DataFrame(inv_boxcox1p(train_scalar2.inverse_transform(x),0.20),columns=y_test_scaled.columns,index=y_test_scaled.index)\n","CE_df.head()"],"metadata":{"id":"9R5A_8uD50y0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test_org  = inv_boxcox1p(y_test,0.20)\n","y_test_org.head()"],"metadata":{"id":"AujLAn2U54_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PERFORMANCE METRICS of the model\n","y1 = mean_absolute_error(y_test_org.iloc[:,0],CE_df.iloc[:,0])\n","print(y1*(1e-9))\n","y2 = mean_absolute_error(y_test_org.iloc[:,1],CE_df.iloc[:,1])\n","print(y2*(1e-9))\n","y3 = mean_absolute_error(y_test_org.iloc[:,2],CE_df.iloc[:,2])\n","print(y3*(1e-9))\n","y4 = mean_absolute_error(y_test_org.iloc[:,3],CE_df.iloc[:,3])\n","print(y4)"],"metadata":{"id":"o-cdkrLX6BVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PERFORMANCE METRICS of the model\n","y1 = np.sqrt(mean_squared_error(y_test_org.iloc[:,0],CE_df.iloc[:,0]))\n","print(y1*(1e-9))\n","y2 = np.sqrt(mean_squared_error(y_test_org.iloc[:,1],CE_df.iloc[:,1]))\n","print(y2*(1e-9))\n","y3 = np.sqrt(mean_squared_error(y_test_org.iloc[:,2],CE_df.iloc[:,2]))\n","print(y3*(1e-9))\n","y4 = np.sqrt(mean_squared_error(y_test_org.iloc[:,3],CE_df.iloc[:,3]))\n","print(y4)"],"metadata":{"id":"LU6xjfqx6bjk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the  model\n","# pickle.dump(CE,open('NNCustEst.pkl','wb'))\n","# pickle.dump(train_scalar2,open('CE_TargetScalar.pkl','wb'))\n","# pickle.dump(train_scalar1,open('CE_FeatureScalar.pkl','wb'))"],"metadata":{"id":"o-4MsCdEK5ly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["MODEL 2 (AVERAGING MODEL)"],"metadata":{"id":"ICuTfkY662Ue"}},{"cell_type":"code","source":["GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01,\n","                                   max_depth=4, max_features='sqrt',\n","                                   min_samples_leaf=15, min_samples_split=10, \n","                                   loss='squared_error', random_state =5)\n","\n","model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n","                             learning_rate=0.01, max_depth=3, \n","                             min_child_weight=1.5, n_estimators=2200,\n","                             reg_alpha=0.4640, reg_lambda=0.8571,\n","                             subsample=0.5213, silent=1,\n","                             random_state =7, nthread = -1)\n","\n","model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n","                              learning_rate=0.01, n_estimators=720,\n","                              max_bin = 55, bagging_fraction = 0.8,\n","                              bagging_freq = 5, feature_fraction = 0.2319,\n","                              feature_fraction_seed=9, bagging_seed=9,\n","                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"],"metadata":{"id":"p5MUmlQO66BT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n","    def __init__(self, models):\n","        self.models = models\n","        self.models2 = models\n","        self.models3 = models\n","        self.models4 = models\n","        \n","    # we define clones of the original models to fit the data in\n","    def fit(self, X, y):\n","        self.models_ = [clone(x) for x in self.models]\n","        self.models2_ = [clone(x) for x in self.models2]\n","        self.models3_ = [clone(x) for x in self.models3]\n","        self.models4_ = [clone(x) for x in self.models4]\n","        self.clone_models = [self.models_,self.models2_,self.models3_,self.models4_]\n","        # Train cloned base models\n","        for model,model2,model3,model4 in zip(self.models_,self.models2_,self.models3_,self.models4_):\n","            model.fit(X, y[0])\n","            model2.fit(X, y[1])\n","            model3.fit(X, y[2])\n","            model4.fit(X, y[3])\n","        return self\n","    \n","    #Now we do the predictions for cloned models and average them\n","    def predict(self, X):\n","        predictions = np.column_stack([\n","            model.predict(X) for model in self.models_\n","        ])\n","        predictions2 = np.column_stack([\n","            model.predict(X) for model in self.models2_\n","        ])\n","        predictions3 = np.column_stack([\n","            model.predict(X) for model in self.models3_\n","        ])\n","        predictions4 = np.column_stack([\n","            model.predict(X) for model in self.models4_\n","        ])\n","        return [np.mean(predictions, axis=1),np.mean(predictions2, axis=1) ,np.mean(predictions3, axis=1) ,np.mean(predictions4, axis=1)]"],"metadata":{"id":"iyA0LL1568zq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CE2 = AveragingModels((model_xgb, GBoost, model_lgb))\n","start_time = time.time()\n","_ = CE2.fit(x_train_scaled,[y_train_scaled.iloc[:,0],y_train_scaled.iloc[:,1],y_train_scaled.iloc[:,2],y_train_scaled.iloc[:,3]])\n","end_time = time.time()\n","print(end_time-start_time)"],"metadata":{"id":"ZmO0UQ9S6_HB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CE2_df = pd.DataFrame(inv_boxcox1p(train_scalar2.inverse_transform(pd.DataFrame(CE2.predict(x_test_scaled)).transpose()),0.20),columns=y_test_scaled.columns,index=y_test_scaled.index)\n","CE2_df.head()"],"metadata":{"id":"vwquvbRa7D0B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test_org = inv_boxcox1p(y_test,0.20)\n","y_test_org.head()"],"metadata":{"id":"SnU6hFZ47JNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y1 = mean_absolute_error(y_test_org.iloc[:,0],CE2_df.iloc[:,0])\n","print(y1*(1e-9))\n","y2 = mean_absolute_error(y_test_org.iloc[:,1],CE2_df.iloc[:,1])\n","print(y2*(1e-9))\n","y3 = mean_absolute_error(y_test_org.iloc[:,2],CE2_df.iloc[:,2])\n","print(y3*(1e-9))\n","y4 = mean_absolute_error(y_test_org.iloc[:,3],CE2_df.iloc[:,3])\n","print(y4)"],"metadata":{"id":"hNm7O7Mh7VMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PERFORMANCE METRICS of the model\n","y1 = np.sqrt(mean_squared_error(y_test_org.iloc[:,0],CE2_df.iloc[:,0]))\n","print(y1*(1e-9))\n","y2 = np.sqrt(mean_squared_error(y_test_org.iloc[:,1],CE2_df.iloc[:,1]))\n","print(y2*(1e-9))\n","y3 = np.sqrt(mean_squared_error(y_test_org.iloc[:,2],CE2_df.iloc[:,2]))\n","print(y3*(1e-9))\n","y4 = np.sqrt(mean_squared_error(y_test_org.iloc[:,3],CE2_df.iloc[:,3]))\n","print(y4)"],"metadata":{"id":"k72ik19u7sLO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pickle.dump(CE2,open('AVGCustEst.pkl','wb'))\n","# pickle.dump(train_scalar2,open('CE_TargetScalar.pkl','wb'))\n","# pickle.dump(train_scalar1,open('CE_FeatureScalar.pkl','wb'))"],"metadata":{"id":"UIp8uz0-L0An"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Combined Model"],"metadata":{"id":"lS_sV4Xw8F3V"}},{"cell_type":"code","source":["probs = np.linspace(0.1,1,10)[:-1]\n","probs"],"metadata":{"id":"9ewsiNkAN9kH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DISK_maes = []\n","SIM_maes = []\n","RES_maes = []\n","TIME_maes = []\n","\n","for prob in probs:\n","  temp_mix = CE_df.multiply(prob).add(CE2_df.multiply(1-prob))\n","  DISK_maes.append(mean_absolute_error(y_test_org.iloc[:,0],temp_mix.iloc[:,0]))\n","  SIM_maes.append(mean_absolute_error(y_test_org.iloc[:,1],temp_mix.iloc[:,1]))\n","  RES_maes.append(mean_absolute_error(y_test_org.iloc[:,2],temp_mix.iloc[:,2]))\n","  TIME_maes.append(mean_absolute_error(y_test_org.iloc[:,3],temp_mix.iloc[:,3]))"],"metadata":{"id":"UFG7M5ezMbZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Final_Mix = CE_df.multiply(0.3).add(CE2_df.multiply(0.7))\n","Final_Mix.head()"],"metadata":{"id":"fbdqOumO8HFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(abs((y_test_org.iloc[:,0] - CE_df.iloc[:,0]).mean()))\n","print(abs((y_test_org.iloc[:,0] - CE2_df.iloc[:,0]).mean()))\n","print(abs((y_test_org.iloc[:,0] - Final_Mix.iloc[:,0]).mean()))"],"metadata":{"id":"1cRKit7z8IDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('CE_df peakdisk',r2_score(y_test_org.iloc[:,0],CE_df.iloc[:,0]))\n","print('CE2_df peakdisk',r2_score(y_test_org.iloc[:,0],CE2_df.iloc[:,0]))\n","print('Final_Mix peakdisk',r2_score(y_test_org.iloc[:,0],Final_Mix.iloc[:,0]))\n","print(' ')\n","print('CE_df peakSim',r2_score(y_test_org.iloc[:,1],CE_df.iloc[:,1]))\n","print('CE2_df peakSim',r2_score(y_test_org.iloc[:,1],CE2_df.iloc[:,1]))\n","print('Final_Mix peakSim',r2_score(y_test_org.iloc[:,1],Final_Mix.iloc[:,1]))\n","print(' ')\n","print('CE_df peakRes',r2_score(y_test_org.iloc[:,2],CE_df.iloc[:,2]))\n","print('CE2_df peakRes',r2_score(y_test_org.iloc[:,2],CE2_df.iloc[:,2]))\n","print('Final_Mix peakRes',r2_score(y_test_org.iloc[:,2],Final_Mix.iloc[:,2]))\n","print(' ')\n","print('CE_df totalTime',r2_score(y_test_org.iloc[:,3],CE_df.iloc[:,3]))\n","print('CE2_df totalTime',r2_score(y_test_org.iloc[:,3],CE2_df.iloc[:,3]))\n","print('Final_Mix totalTime',r2_score(y_test_org.iloc[:,3],Final_Mix.iloc[:,3]))"],"metadata":{"id":"j8Pb5Q2H8Tjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = ['RandomForest','XGBoost','NeuralNetwork','AverageRegressor','Combined']\n","y1 = [20830591385.6,12745315450.88,mean_absolute_error(y_test_org.iloc[:,0],CE_df.iloc[:,0]),mean_absolute_error(y_test_org.iloc[:,0],CE2_df.iloc[:,0]),mean_absolute_error(y_test_org.iloc[:,0],Final_Mix.iloc[:,0])]\n","y1 = [y*(1e-9) for y in y1]\n","y2 = [3221225472,5905580032,mean_absolute_error(y_test_org.iloc[:,1],CE_df.iloc[:,1]),mean_absolute_error(y_test_org.iloc[:,1],CE2_df.iloc[:,1]),mean_absolute_error(y_test_org.iloc[:,1],Final_Mix.iloc[:,1])]\n","y2 = [y*(1e-9) for y in y2]\n","y3 = [4294967296,3435973836.8,mean_absolute_error(y_test_org.iloc[:,2],CE_df.iloc[:,2]),mean_absolute_error(y_test_org.iloc[:,2],CE2_df.iloc[:,2]),mean_absolute_error(y_test_org.iloc[:,2],Final_Mix.iloc[:,2])]\n","y3 = [y*(1e-9) for y in y3]\n","y4 = [4300,5000,mean_absolute_error(y_test_org.iloc[:,3],CE_df.iloc[:,3]),mean_absolute_error(y_test_org.iloc[:,3],CE2_df.iloc[:,3]),mean_absolute_error(y_test_org.iloc[:,3],Final_Mix.iloc[:,3])]"],"metadata":{"id":"nfc56eJ08VyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.rcParams[\"figure.figsize\"] = (20,10)\n","fig,axs = plt.subplots(2,2)\n","\n","axs[0,0].plot(x, y1,'go-', label='line 1', linewidth=2)\n","axs[0,0].set_title('Peak Disk Usage')\n","# axs[0,0].set_xlabel('Models')\n","axs[0,0].set_ylabel('MAE Loss (Test) in GB')\n","# axs[0,0].legend(['train', 'val'], loc='upper left')\n","\n","axs[0,1].plot(x, y2, 'bo-', label='line 1', linewidth=2)\n","axs[0,1].set_title('Peak Simulation RAM')\n","# axs[0,1].set_xlabel('Models')\n","axs[0,1].set_ylabel('MAE Loss (Test) in GB')\n","# axs[0,0].legend(['train', 'val'], loc='upper left')\n","\n","axs[1,0].plot(x, y3, 'ro-', label='line 1', linewidth=2)\n","axs[1,0].set_title('Peak Results Parsing RAM')\n","# axs[1,0].set_xlabel('Models')\n","axs[1,0].set_ylabel('MAE Loss (Test) in GB')\n","# axs[0,0].legend(['train', 'val'], loc='upper left')\n","\n","axs[1,1].plot(x, y4, 'yo-', label='line 1', linewidth=2)\n","axs[1,1].set_title('Total Time Taken')\n","# axs[1,1].set_xlabel('Models')\n","axs[1,1].set_ylabel('MAE Loss (Test) in Secs')\n","# axs[0,0].legend(['train', 'val'], loc='upper left')"],"metadata":{"id":"yHVRXN3k8e1Y"},"execution_count":null,"outputs":[]}]}